/*
========================================================================================
    nf-core/elemutils Nextflow base config file
========================================================================================
    A 'blank slate' config file, appropriate for general use on most high performance
    compute environments. Assumes that all software is installed and available on
    the PATH. Runs in `local` mode - all jobs will be run on the logged in environment.
----------------------------------------------------------------------------------------
*/

process {

    // TODO nf-core: Check the defaults for all processes
    cpus   = { check_max( 1    * task.attempt, 'cpus'   ) }
    memory = { check_max( 6.GB * task.attempt, 'memory' ) }
    time   = { check_max( 4.h  * task.attempt, 'time'   ) }

    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries    = 1
    maxErrors     = '-1'

    // Process-specific resource requirements
    // NOTE - Please try and re-use the labels below as much as possible.
    //        These labels are used and recognised by default in DSL2 files hosted on nf-core/modules.
    //        If possible, it would be nice to keep the same label naming convention when
    //        adding in your local modules too.
    // TODO nf-core: Customise requirements for specific processes.
    // See https://www.nextflow.io/docs/latest/config.html#config-process-selectors
    withLabel:process_low {
        cpus   = { check_max( 2     * task.attempt, 'cpus'    ) }
        memory = { check_max( 12.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 4.h   * task.attempt, 'time'    ) }
    }
    withLabel:process_medium {
        cpus   = { check_max( 6     * task.attempt, 'cpus'    ) }
        memory = { check_max( 36.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 8.h   * task.attempt, 'time'    ) }
    }
    withLabel:process_high {
        cpus   = { check_max( 12    * task.attempt, 'cpus'    ) }
        memory = { check_max( 72.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 16.h  * task.attempt, 'time'    ) }
    }
    withLabel:gpu {
        accelerator = 1
        cpus   = { check_max( 32    * task.attempt, 'cpus'    ) }
        memory = { check_max( 128.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 3.h  * task.attempt, 'time'    ) }
    }
    withLabel:process_long {
        time   = { check_max( 20.h  * task.attempt, 'time'    ) }
    }
    withLabel:process_high_memory {
        memory = { check_max( 200.GB * task.attempt, 'memory' ) }
    }
    withLabel:error_ignore {
        errorStrategy = 'ignore'
    }
    withLabel:error_retry {
        errorStrategy = 'retry'
        maxRetries    = 2
    }

    ext {
        aws {
            resourceRequirements = [
                [type: 'GPU', value: '0']
            ]
        }
    }

    withName: PARABRICKS_FQ2BAM {
        accelerator = 1
        cpus = 8
        memory = 28.GB
        time = 2.h
        maxRetries = 3
        maxForks = (params.batch_size != -1) ? params.batch_size : 12
    }
    
    withName: 'PARABRICKS_DEEPVARIANT' {
        accelerator = 1
        cpus = 8
        memory = 28.GB
        time = 2.h
        maxRetries = 3
        maxForks = (params.batch_size != -1) ? params.batch_size : 12
    }

    withName: 'TABIX_BGZIPTABIX' {
        cpus   = { check_max( 1 * task.attempt, 'cpus' ) }
        memory = { check_max( 2.GB * task.attempt, 'memory' ) }
        time = { check_max( 1.h * task.attempt, 'time' ) }
        maxRetries = 3
    }

    withName: 'MULTIQC' {
        cpus   = { check_max( 2 * task.attempt, 'cpus' ) }
        memory = { check_max( 8.GB * task.attempt, 'memory' ) }
        time = { check_max( 1.h * task.attempt, 'time' ) }
        maxRetries = 3
    }
}

aws {
   batch {
      cliPath = '/home/ec2-user/miniconda/bin/aws'
      maxParallelTransfers = 48
      maxTransferAttempts = 3
   }
   client {
      maxConnections = 48
      uploadMaxThreads = 48
      maxErrorRetry = 3
      socketTimeout = 3600000
      uploadRetrySleep = 100
      uploadChunkSize = 32.MB
   }
   batch {
      maxParallelTransfers = 48
   }
}


docker {
    enabled = true
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
